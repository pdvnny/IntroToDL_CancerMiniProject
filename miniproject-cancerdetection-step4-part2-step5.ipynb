{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"miniproject-cancerdetection-step4-part2-step5.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOQQRDyvqCyedqJQnhWmsUe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Parker Dunn\n","\n","__Assignment for COURSERA: Introduction to Deep Learning (via CU Boulder)__  \n","__Assignment:__ Week 3 - CNN Cancer Detection Kaggle Mini-Project\n","\n","# Step 4 (cont.)"],"metadata":{"id":"YfLo1Rp9_Xp-"}},{"cell_type":"markdown","source":["## Step 4 - Part 6 - Model optimization and discussion of testing\n","\n","* Analysis of why or why not something worked\n","* Hyperparameter optimiation procedure summary"],"metadata":{"id":"47q2KOuiAFO6"}},{"cell_type":"markdown","source":["### Hyperparameter optimization\n","\n","I used similar procedure for hyperparemeter testing that I did for testing model designs. \n","\n","I divided up the training images into training and validation sets. I trained the model (with my chosen design) using three learning rates and 5 epochs each time. I assessed the training process by plotting the training and validation accuracy across the 5 epochs.\n","\n","Analayzing the training graphs, I selected a learning rate that appeared to improve training and validation accuracy consistently. I was looking for a training procedure that would yield gradual improvements toward an optimal solution rather than high accuracy results that might overfit the training and validation data.\n","\n","### Analysis of model testing\n","\n","I was able to train a model that achieved greater than 80% training and validation accuracy using a learning rate of 0.001 and 4 epochs. This seemed to be the best combination of parameters to use for training.\n","\n","After preparing the model on all the training data, the model achieved greater than 90% accuracy during the last epoch. However, when evaluating on the test data, the overall score was very low -- approx. 0.51.\n","\n","My initial idea to explain this low test performance is overfitting to the images that I have been experimenting and training with over and over. I have not worked with many datasets as large as this, so I was unsure about how much data to use at each stage of model development. Normally, it is standard to use as much data as possible. Images of cells, in my opinion, are not very complex though and tend to be repetitive, so I was consistently concerned about overfitting and model complexity.\n","\n"],"metadata":{"id":"Jr18pgxRA_ze"}},{"cell_type":"markdown","source":["# Step 5 - Conclusion\n","\n","I prematurely ended my efforts to develop a great model. I wanted to spend a reasonable amount of time and set reasonable expectations, so I stopped tweaking my design even though I knew I could probably achieve a better design. For my first significant deep learning project, it seemed too drastic to try to re-invent a model like ResNet or LeNet. As a result, the testing performance of my model was not great (see image with my Kaggle results - \"parkerdunn_kaggle_submission.jpg\").\n","\n","It was a great experience despite the poor results. This was one of the largest ML projects that I pursued so far with regard to size of the model, size of the data, and time invested into model development. It was a valuable lesson about the the model development process, and I know that I will achieve better models in the future as I leave myself more time to work on tuning and design testing. I found myself at times struggling at non-essential steps do to lack of experience and familiarity with this process. For example, I knew I would not be able to load all of the images into a table format at once (I tried anyway, but was pretty sure it wouldn't work), but it took me some time to figure out a strong way to work with the images. I feel like I have a much better sense now of how to prepare and work with data for a deep learning project.\n","\n","I gained a much better appreciation for regularization techniques during this project. In previous work, I hadn't worked with a dataset and deep model that demonstrated the importance of dropout layers. In this project, it was evident during the training and valudation (while testing designs) that dropout layers did help with overfitting.\n","\n","Another big lesson that I learned was to setup consistent ways to visualize model design and performance. In addition to realizing the ease and value of plotting performance, I prohibited my progress at times by not double-checking my model designs with `model.summary()`. It seemes to me that there is no reason not to use it each time a model is created for testing, and I will certainly use it in the future to avoid designing a model in an unintended manner.\n","\n","\n","\n"],"metadata":{"id":"ZqxMrLLhwHpj"}}]}
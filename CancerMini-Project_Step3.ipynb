{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1628d7aa-f085-4c32-b677-11a5a3441b44",
   "metadata": {},
   "source": [
    "# Parker Dunn\n",
    "\n",
    "__Assignment for COURSERA: Introduction to Deep Learning (via CU Boulder)__  \n",
    "__Assignment:__ Week 3 - CNN Cancer Detection Kaggle Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb6ebe-f2ec-4a88-be60-c557bbb34435",
   "metadata": {},
   "source": [
    "## Section 3 - Model Architecture\n",
    "\n",
    "#### Plan\n",
    "\n",
    "Due to limited time and computing resources, I'll stick to a simple model. I plan to use the \"building block-style\" Covolution-Convolution-Pooling design pattern with probably no more than 4 repetitions of this pattern. Since we previously experimented with the development of neural network architecture, I am hoping to replicate a reliable NN structure from one of the example image classification models from the videos. In theory, the key features will be extracted by the convolution architecture and the NN structure from another image classification task can be successfully optimized for new features.\n",
    "\n",
    "Laid out below are my achitecture plans as well as some of the thoughts I have regarding the training of my CNN.\n",
    "\n",
    "__Design parameters and Hyperparameters__\n",
    "Decisions\n",
    "* I will use ReLU (hidden layers) and sigmoid (output layer) as activation functions. This is not a design parameter that I plan to vary this time.\n",
    "* I will primarily use 3x3xd convolutional filters\n",
    "* As an optimization method, I will stick to SGD, which I am most familiar with, and plant to incorporate momentum if possible with the Keras API.\n",
    "\n",
    "Hyperparameters\n",
    "* Learning rate\n",
    "    * Test: 0.01 | 0.001 | 0.0001 (3 values)\n",
    "* Momentum\n",
    "    * Test: 0.0 | 0.01 | 0.1 (3 values)\n",
    "* Number of epochs (i.e., how much training)\n",
    "\n",
    "Design\n",
    "* Number of [Conv-Conv-Pool] layers\n",
    "    - Test: 2, 3, 4\n",
    "* Number of filters to use\n",
    "\n",
    "Potential ways to improve a struggling model\n",
    "* L2 regularization\n",
    "* Batch normalization\n",
    "\n",
    "I plan to use moderate training parameters at first (e.g. learning rate -> 0.001 and momentum -> 0.01) to experiment and narrow down some viable convolution designs.\n",
    "\n",
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f28d1bc-a65b-45d4-9b37-3365a7820717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def partial_load_data(n):\n",
    "    # n == total number of images to load\n",
    "    # split == tuple with fraction of images for training and validation\n",
    "    \n",
    "    train_locs, test_locs, y_train_info = load_image_info()\n",
    "    \n",
    "    # Generate random set of indices\n",
    "    rand_idx = np.random.randint(0,200000,(n,))\n",
    "    #print(len(train_locs))\n",
    "    #print(rand_idx.shape)\n",
    "    \n",
    "    X = np.zeros((n, 96, 96, 3))\n",
    "    X_IDs = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        ind = rand_idx[i]\n",
    "        #print(i, ind)\n",
    "        img_file = train_locs[ind]\n",
    "        img = io.imread(img_file)        # NOTE: io.imread() reads images in as numpy.ndarray\n",
    "        \n",
    "        #img = img.reshape(1,96*96,3)\n",
    "        \n",
    "        X[i,:,:,:] = img /255.0  # NOTE: MODIFYING ALL VALUES TO 0-1 SCALE!!!\n",
    "        \n",
    "        X_IDs.append(img_file[6:-4])\n",
    "    \n",
    "    return X, X_IDs, y_train_info\n",
    "\n",
    "def partial_train_val_split(X, y_info, split=(0.66, 0.34)):\n",
    "    # generate indices for training and validations sets based on 'split'\n",
    "    sz = len(X)\n",
    "    n_train, n_val = [int(split[i] * sz) for i in range(len(split))]\n",
    "    print(n_train, n_val)\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "    idx_train = rng.choice(range(sz), (n_train,), replace=False, shuffle=False)\n",
    "    idx_val = list(set(range(sz)) - set(idx_train))\n",
    "    \n",
    "    # separate X and y_info into separate datasets\n",
    "    X_tr = X[idx_train,:,:,:]\n",
    "    y_tr = y_info.iloc[idx_train,:].reset_index()\n",
    "    \n",
    "    X_val = X[idx_val,:,:,:]\n",
    "    y_val = y_info.iloc[idx_val,:].reset_index()\n",
    "    \n",
    "    # return X_tr, y_tr, X_val, y_val\n",
    "    return X_tr, y_tr, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795f97d-77f7-43ca-8089-e11b94130c99",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 3 - Part 1: Trying to find a repeatable way to create a CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60cde3b7-3430-4cd6-a0b6-783aa90854d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "#import numpy as np\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0de44dac-77b2-457c-a4b8-d7fa5f681f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A FUNCTION THAT TAKES THE PARAMETERS ABOVE AND TURNS THEM INTO A MODEL!\n",
    "\n",
    "def model_creator(layers_lst, layer_design):\n",
    "    model = tf.keras.Sequential()\n",
    "    for (l, d) in zip(layers_lst, layer_design):\n",
    "        if l == \"input\":\n",
    "            model.add(layers.Conv2D(d[\"filters\"], d[\"kernel_size\"], padding=d[\"padding\"], use_bias=d[\"use_bias\"], input_shape=d[\"input_shape\"]))\n",
    "        elif l == \"conv\":\n",
    "            model.add(layers.Conv2D(d[\"filters\"], d[\"kernel_size\"], padding=d[\"padding\"], use_bias=d[\"use_bias\"]))\n",
    "        elif l == \"maxpool\":\n",
    "            model.add(layers.MaxPool2D(d[\"pool_size\"]))\n",
    "        elif l == \"flatten\":\n",
    "            model.add(layers.Flatten())\n",
    "        elif l == \"dense\":\n",
    "            model.add(layers.Dense(d[\"size\"], activation=d[\"activation\"]))\n",
    "        # elif l == \"output\":\n",
    "        #     model.add(layers.Dense(d[\"size\"], activation=d[\"activation\"])\n",
    "        else:\n",
    "            raise Exception(\"Invalid layer provided for the model\")\n",
    "    return model\n",
    "        \n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ccaf1d-2a4f-4a8d-888d-8b1062914be4",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### Step 3 - Part 2: Loading some image data and splitting into training and validation\n",
    "\n",
    "I don't want to use all of the available images to do some preliminary testing of model designs. Therefore, I'll setup some specific functions for training and validating on a small subset of the images available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821666eb-9d00-4733-bb65-2a95f89380fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980 1020\n",
      "CPU times: total: 3.66 s\n",
      "Wall time: 6.64 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1007</td>\n",
       "      <td>fa2c542f9a83215af026d653b76efa7bb23c109a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>414</td>\n",
       "      <td>9d9789359296f108630ce9331b15e4fb49c543f8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>341</td>\n",
       "      <td>8c23e23c8f19c10781dd69dea64c8a46b70e57b9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>498</td>\n",
       "      <td>424845b0f9cae537878b4ebeed9acc1965ff1721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>86b5de79c16d0ff8f1df0eeb6e1684e3f9467caf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                        id  label\n",
       "0   1007  fa2c542f9a83215af026d653b76efa7bb23c109a      1\n",
       "1    414  9d9789359296f108630ce9331b15e4fb49c543f8      0\n",
       "2    341  8c23e23c8f19c10781dd69dea64c8a46b70e57b9      0\n",
       "3    498  424845b0f9cae537878b4ebeed9acc1965ff1721      1\n",
       "4    306  86b5de79c16d0ff8f1df0eeb6e1684e3f9467caf      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 3000 samples -> 9.91 s\n",
    "# 5000 samples -> 15.5 s\n",
    "\n",
    "# MOVED LOADING OF DATA TO BELOW\n",
    "\"\"\"\n",
    "X, X_ids, y_info = partial_load_data(5000)\n",
    "X_tr, y_tr, X_val, y_val = partial_train_val_split(X, y_info, split=(0.6, 0.4))\n",
    "y_tr.head(5)\n",
    "\"\"\"\n",
    "\n",
    "## Just double-checking the X_tr, and X_val data\n",
    "\n",
    "\"\"\"\n",
    "print(y_tr.describe(),\"\\n\")\n",
    "#print(y_val.describe())\n",
    "print(X_tr[0,:,:,:].shape, \"\\n\")\n",
    "print(X_val.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805dc61-fbe0-47b3-9dac-68795634378e",
   "metadata": {},
   "source": [
    "#### Step 3 - Part 3: Testing model parameters\n",
    "\n",
    "- [Done] Load some sample data\n",
    "- [Done] Create a consistent way to create CNN object\n",
    "- [Done] Create function to perform final steps of model design\n",
    "- [Done] Training and validation (I believe this is a TensorFlow built in function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62fbd14c-88ae-48fc-b330-517907f37816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compiler(model, opt_params, metrics):\n",
    "    opt = optimizers.SGD(learning_rate=opt_params[0],\n",
    "                         momentum=opt_params[1],\n",
    "                        name='SGD')\n",
    "    # \"from_logits=True\" -- recommended by tf documentation\n",
    "    model.compile(optimizer=opt,\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428c1c9a-791d-4eaf-9f98-fc76fab369bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_lst = [\"input\",\"conv\", \"maxpool\",\"conv\",\"conv\",\"maxpool\",\"flatten\",\"dense\",\"dense\",\"dense\"]\n",
    "layer_design = [\n",
    "    {\"filters\":24, \"kernel_size\":(3,3), \"padding\":\"valid\", \"data_format\":\"channels_last\", \"use_bias\":True, \"input_shape\":(96,96,3)},\n",
    "    {\"filters\":48, \"kernel_size\":(3,3), \"padding\":\"valid\", \"data_format\":\"channels_last\", \"use_bias\":True},\n",
    "    {\"pool_size\":(2,2)},\n",
    "    {\"filters\":64, \"kernel_size\":(3,3), \"padding\":\"valid\", \"data_format\":\"channels_last\", \"use_bias\":True},\n",
    "    {\"filters\":72, \"kernel_size\":(3,3), \"padding\":\"valid\", \"data_format\":\"channels_last\", \"use_bias\":True},\n",
    "    {\"pool_size\":(2,2)},\n",
    "    None,\n",
    "    {\"size\":96, \"activation\":'relu'},\n",
    "    {\"size\":48, \"activation\":'relu'},\n",
    "    {\"size\":1, \"activation\":'sigmoid'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04f70324-e59e-42db-8d9d-a2bdc83456d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980 1020\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 2s 109ms/step - loss: 0.6819 - accuracy: 0.5747 - val_loss: 0.6719 - val_accuracy: 0.6049\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.6761 - accuracy: 0.5955 - val_loss: 0.6703 - val_accuracy: 0.6049\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.6757 - accuracy: 0.5955 - val_loss: 0.6709 - val_accuracy: 0.6049\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.6754 - accuracy: 0.5955 - val_loss: 0.6710 - val_accuracy: 0.6049\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.6747 - accuracy: 0.5955 - val_loss: 0.6706 - val_accuracy: 0.6049\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 0.6748 - accuracy: 0.5955 - val_loss: 0.6705 - val_accuracy: 0.6049\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.6745 - accuracy: 0.5955 - val_loss: 0.6708 - val_accuracy: 0.6049\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.6745 - accuracy: 0.5955 - val_loss: 0.6722 - val_accuracy: 0.6049\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.6740 - accuracy: 0.5955 - val_loss: 0.6712 - val_accuracy: 0.6049\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.6738 - accuracy: 0.5955 - val_loss: 0.6731 - val_accuracy: 0.6049\n",
      "--- Ending Accuracy --- \n",
      " Validation: 60.49% \n",
      " Training: 59.55% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repeating the random sampling from above\n",
    "X, X_ids, y_info = partial_load_data(3000)\n",
    "X_tr, y_tr, X_val, y_val = partial_train_val_split(X, y_info)\n",
    "\n",
    "# STEPS TO CREATE THE MODEL\n",
    "model = model_creator(layers_lst, layer_design)\n",
    "model = model_compiler(model, (0.001, 0.01), ['accuracy'])\n",
    "\n",
    "# Checking something\n",
    "print(type(y_tr.loc[:,\"label\"].to_numpy()))\n",
    "\n",
    "# history is kind of like results!\n",
    "history = model.fit(X_tr,\n",
    "                    y_tr.loc[:,\"label\"].to_numpy(),\n",
    "                    batch_size=100,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_val, y_val.loc[:,\"label\"].to_numpy())\n",
    "                   )\n",
    "\n",
    "print(\"--- Ending Accuracy ---\",\"\\n\",\n",
    "      f\"Validation: {history.history['val_accuracy'][-1]:.2%}\",\"\\n\",\n",
    "      f\"Training: {history.history['accuracy'][-1]:.2%}\",\"\\n\")\n",
    "\n",
    "now_obj = datetime.now()\n",
    "now_date = now_obj.strftime(\"%Y_%m_%d\")\n",
    "now_time = (now_obj.hour * 3600) + (now_obj.minute * 60) + now_obj.second\n",
    "now = f\"{now_date}_{now_time}\"\n",
    "\n",
    "filename1=f\"{now}_{int(history.history['val_accuracy'][-1] * 100)}.json\"\n",
    "filename2=f\"{now}_layers.json\"\n",
    "filename3=f\"{now}_design.json\"\n",
    "\n",
    "\n",
    "with open(f\"design_testing/{filename1}\", \"w\") as write_file:\n",
    "    model_json = model.to_json()\n",
    "    json.dump(model_json, write_file)\n",
    "    \n",
    "with open(f\"design_testing/{filename2}\", \"w\") as write_file:\n",
    "    json.dump(layers_lst, write_file)\n",
    "\n",
    "with open(f\"design_testing/{filename3}\", \"w\") as write_file:\n",
    "    json.dump(layer_design, write_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

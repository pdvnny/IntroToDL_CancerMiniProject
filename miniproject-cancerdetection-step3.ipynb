{"cells":[{"cell_type":"markdown","metadata":{"id":"1628d7aa-f085-4c32-b677-11a5a3441b44"},"source":["# Parker Dunn\n","\n","__Assignment for COURSERA: Introduction to Deep Learning (via CU Boulder)__  \n","__Assignment:__ Week 3 - CNN Cancer Detection Kaggle Mini-Project"],"id":"1628d7aa-f085-4c32-b677-11a5a3441b44"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":215204,"status":"ok","timestamp":1658849829831,"user":{"displayName":"Parker Dunn","userId":"12849044322373918721"},"user_tz":240},"id":"FAAQzITrUFFh","outputId":"783ac86d-1c7d-401c-b8d0-226f5b04ab5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Working directory...\n","/content\n","Mounted at /content/drive\n","Downloading histopathologic-cancer-detection.zip to /content\n","100% 6.30G/6.31G [01:44<00:00, 68.2MB/s]\n","100% 6.31G/6.31G [01:44<00:00, 65.1MB/s]\n"]}],"source":["# CONNECTING TO GOOGLE DRIVE & KAGGLE DATA\n","\n","import os\n","import shutil\n","\n","print(\"Working directory...\")\n","!pwd\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","os.environ['KAGGLE_CONFIG_DIR'] = \"drive/MyDrive\"\n","!kaggle competitions download -c histopathologic-cancer-detection\n","!unzip -q histopathologic-cancer-detection.zip"],"id":"FAAQzITrUFFh"},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZm0WcBNucGy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658850258486,"user_tz":240,"elapsed":393911,"user":{"displayName":"Parker Dunn","userId":"12849044322373918721"}},"outputId":"7fcd65f1-9e3f-4d35-b75a-3c4ac15af918"},"outputs":[{"output_type":"stream","name":"stdout","text":["a691ec5370de90eba32987009ec3ce44ab035ce8.tif\n","89b91e9ddcb436bc58449f6d02d3a6b1ae111fcc.tif\n","f2cab828677885357d217aa382736c9f57b67a31.tif\n","de73030a1700950008cbb063be83e36c478d4eea.tif\n","5271fa4a502af680cd688abc1b38efdf2976ecce.tif\n","(96, 96, 3)\n","test/a691ec5370de90eba32987009ec3ce44ab035ce8.jpg\n","(96, 96, 3)\n","test/89b91e9ddcb436bc58449f6d02d3a6b1ae111fcc.jpg\n","(96, 96, 3)\n","test/f2cab828677885357d217aa382736c9f57b67a31.jpg\n"]}],"source":["# RE-ARRANGING THE IMAGE FILES TO BE COMPATIBLE WITH tf.keras.utils.images_from_directory...\n","\n","!mkdir train/pos\n","!mkdir train/neg\n","\n","import pandas as pd\n","import numpy as np\n","from skimage import io\n","\n","y_info = pd.read_csv(\"train_labels.csv\", header=0)\n","\n","for i in range(y_info.shape[0]):\n","  f = y_info.loc[i,\"id\"]\n","  img = io.imread(\"train/\"+f+\".tif\")\n","\n","  if y_info.loc[i,\"label\"]:   # \"== 1\"\n","    io.imsave(\"train/pos/\"+f+\".jpg\", img, check_contrast=False, quality=100)\n","    os.remove(\"train/\"+f+\".tif\")\n","  else:\n","    os.remove(\"train/\"+f+\".tif\")\n","    io.imsave(\"train/neg/\"+f+\".jpg\", img, check_contrast=False, quality=100)\n","\n","test_locs = os.listdir(\"test/\")\n","\n","for i in range(5):\n","  print(test_locs[i])\n","\n","count = 0\n","for ff in test_locs:\n","    img = io.imread(\"test/\"+ff)\n","    if (count < 3):\n","      print(img.shape)\n","      print(f\"test/{ff[:-4]}.jpg\")\n","      count += 1\n","    io.imsave(\"test/\"+ff[:-4]+\".jpg\", img, check_contrast=False, quality=100)\n","    os.remove(\"test/\"+ff)"],"id":"JZm0WcBNucGy"},{"cell_type":"markdown","metadata":{"id":"fdeb6ebe-f2ec-4a88-be60-c557bbb34435"},"source":["___\n","## Section 3 - Model Architecture\n","\n","#### Plan\n","\n","I'll stick to a simple model. I plan to use the \"building block-style\" Covolution-Convolution-Pooling design pattern with probably no more than 4 repetitions of this pattern. Since we previously experimented with the development of neural network architecture, I am hoping to replicate a reliable NN structure from one of the example image classification models from the videos. In theory, the key features will be extracted by the convolution architecture and the NN structure from another image classification task can be successfully optimized for new features.\n","\n","Laid out below are my achitecture plans as well as some of the thoughts I have regarding the training of my CNN.\n","\n","__Design parameters and Hyperparameters__\n","Decisions\n","* I will use ReLU (hidden layers) and sigmoid (output layer) as activation functions. This is not a design parameter that I plan to vary this time.\n","* I will primarily use 3x3 convolutional filters\n","* As an optimization method, I will stick to SGD with momentum because I am most familiar with SGD\n","\n","--- __*Update*__ ---\n","* Decided to change optimizer: SGD -> Adam\n","  - I was looking for ways to improve my results and Adam was suggested\n","  - I'm going to use the default values for the parameters that I am not familiar with: \"beta_1\", \"beta_2\", \"epsilon\"\n","\n","Hyperparameters\n","* Learning rate\n","    * Test: 0.01 | 0.001 | 0.0001 (3 values)\n","* Momentum\n","    * Test: 0.0 | 0.01 | 0.1 (3 values)\n","* Number of epochs (i.e., how much training)\n","\n","Design\n","* Number of [Conv-Conv-Pool] layers\n","    - Test: 2, 3, 4\n","* Number of filters to use\n","\n","Potential ways to improve a struggling model\n","* L2 regularization\n","* Batch normalization\n","\n","I plan to use moderate training parameters at first (e.g. learning rate -> 0.001 and momentum -> 0.01) to experiment and narrow down some viable convolution designs."],"id":"fdeb6ebe-f2ec-4a88-be60-c557bbb34435"},{"cell_type":"markdown","metadata":{"id":"d795f97d-77f7-43ca-8089-e11b94130c99"},"source":["___\n","#### Step 3 - Part 1: Loading packages, defining variables, and loading data\n","\n","Trying to find a repeatable way to create a CNN!"],"id":"d795f97d-77f7-43ca-8089-e11b94130c99"},{"cell_type":"code","execution_count":null,"metadata":{"id":"60cde3b7-3430-4cd6-a0b6-783aa90854d6"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models, optimizers\n","\n","from datetime import datetime\n","import json\n","import pathlib\n","\n","# import os\n","# import shutil\n","# from skimage import io\n","import pandas as pd\n","import numpy as np\n","\n","import glob\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","\n","# CRITICAL VARIABLES\n","\n","PATH=\"drive/MyDrive/IntroToDL_CancerMiniProject/\"\n","TRAIN = pathlib.Path(\"train/\")\n","TEST= pathlib.Path(\"test/\")\n","\n","batch_size=64\n","img_dim=96\n","\n","AUTOTUNE = tf.data.AUTOTUNE"],"id":"60cde3b7-3430-4cd6-a0b6-783aa90854d6"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16956,"status":"ok","timestamp":1658850470130,"user":{"displayName":"Parker Dunn","userId":"12849044322373918721"},"user_tz":240},"id":"Y1azwFAms8Ch","outputId":"b9640cca-e0f7-4889-9e64-609d57d52044"},"outputs":[{"output_type":"stream","name":"stdout","text":["Image Count: 220025\n","Found 220025 files belonging to 2 classes.\n","Using 147417 files for training.\n","Found 220025 files belonging to 2 classes.\n","Using 72608 files for validation.\n","(64, 96, 96, 3)\n","(64,)\n","68005f6f36d29506e5dbfa20228fc100c7e12d9e.jpg\n","c6633e871be12066e21cacbef9fc792eed9de54f.jpg\n","ba94d2cb8d5d96227426b840b50561bcab495848.jpg\n","b03aff0f0d2c4391f72a32fac31239440729bf36.jpg\n"]}],"source":["# Loading the image data\n","\n","train_locs=glob.glob(\"train/*/*.jpg\")\n","image_count = len(train_locs)\n","print(f\"Image Count: {image_count}\")\n","\n","\n","train_ds = tf.keras.utils.image_dataset_from_directory(\n","    TRAIN,\n","    validation_split=0.33,\n","    subset=\"training\",\n","    seed=123,\n","    image_size=(img_dim, img_dim),\n","    batch_size=batch_size)\n","\n","val_ds = tf.keras.utils.image_dataset_from_directory(\n","    TRAIN,\n","    validation_split=0.33,\n","    subset=\"validation\",\n","    seed=123,\n","    image_size=(img_dim, img_dim),\n","    batch_size=batch_size)\n","\n","# test_ds = tf.keras.utils.image_dataset_from_directory(\n","#     TEST,\n","#     image_size=(img_dim, img_dim))\n","\n","for image_batch, labels_batch in train_ds:\n","  print(image_batch.shape)\n","  print(labels_batch.shape)\n","  break\n","\n","# for image_batch, labels_batch in test_ds:\n","#   print(image_batch.shape)\n","#   print(labels_batch.shape)\n","#   break\n","\n","test_dir_files = os.listdir(TEST)\n","for i in range(4):\n","  print(test_dir_files[i])"],"id":"Y1azwFAms8Ch"},{"cell_type":"markdown","source":["#### Step 3 - Part 2: Preparing functions to build model and preparing for training\n","\n","* I tried to create a function that would allow me to quickly modify a list to change model parameters\n","* I normalized the images to a [0,1] scale\n","* I cropped the images since only the middle 32x32 pixel region of the images is used for classification\n","* I created a function to perform the model compilation process.\n"],"metadata":{"id":"kf0fPWRwFRFO"},"id":"kf0fPWRwFRFO"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0de44dac-77b2-457c-a4b8-d7fa5f681f67"},"outputs":[],"source":["## A FUNCTION THAT TAKES THE PARAMETERS ABOVE AND TURNS THEM INTO A MODEL!\n","\n","def model_creator(layers_lst, layer_design):\n","    model = tf.keras.Sequential()\n","    model.add(layers.Rescaling(1./255))\n","    model.add(layers.Cropping2D(cropping=((12,12),(12,12)), data_format='channels_last'))\n","\n","    for (l, d) in zip(layers_lst, layer_design):\n","        if l == \"input\":\n","            model.add(layers.Conv2D(d[\"filters\"], d[\"kernel_size\"], activation='relu', padding=d[\"padding\"], input_shape=d[\"input_shape\"]))\n","        elif l == \"conv\":\n","            model.add(layers.Conv2D(d[\"filters\"], d[\"kernel_size\"], activation='relu', padding=d[\"padding\"]))\n","        elif l == \"maxpool\":\n","            model.add(layers.MaxPool2D(d[\"pool_size\"], strides=d[\"strides\"]))\n","        elif l == \"flatten\":\n","            model.add(layers.Flatten())\n","        elif l == \"dense\":\n","            model.add(layers.Dense(d[\"size\"], activation=d[\"activation\"]))\n","        elif l == \"output\":\n","            model.add(layers.Dense(d[\"size\"]))\n","        elif l == \"dropout\":\n","            model.add(layers.Dropout(d[\"rate\"]))\n","        elif l == \"norm\":\n","            model.add(layers.BatchNormalization(axis=3))  # not 100 percent sure 3 is correct\n","        elif l == \"conv_pool\":\n","            model.add(layers.Conv2D(d[\"filters\"], d[\"kernel_size\"], strides=d[\"strides\"], padding=d['padding']))\n","        else:\n","            raise Exception(\"Invalid layer provided for the model\")\n","\n","    return model\n","        \n","# model.summary()\n","\n","def model_compiler(model, opt_param, metrics):\n","    opt = optimizers.Adam(learning_rate=opt_param)\n","    # \"from_logits=True\" -- recommended by tf documentation\n","    model.compile(optimizer=opt,\n","                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n","                  metrics=metrics)\n","    return model"],"id":"0de44dac-77b2-457c-a4b8-d7fa5f681f67"},{"cell_type":"markdown","metadata":{"id":"4805dc61-fbe0-47b3-9dac-68795634378e"},"source":["#### Step 3 - Part 3: Testing model parameters"],"id":"4805dc61-fbe0-47b3-9dac-68795634378e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"428c1c9a-791d-4eaf-9f98-fc76fab369bc"},"outputs":[],"source":["layers_lst = [\"input\",\n","              \"conv_pool\",\n","              \"norm\",\n","              \"conv\",\n","              \"conv\",\n","              \"maxpool\",\n","              \"norm\",\n","              \"conv\",\n","              \"conv\",\n","              \"maxpool\",\n","              \"norm\",\n","              \"flatten\",\n","              \"dense\",\n","              \"dropout\",\n","              \"dense\",\n","              \"dropout\",\n","              \"dense\"\n","              ]\n","layer_design = [\n","    {\"filters\":16, \"kernel_size\":(7,7), \"padding\":\"valid\", \"input_shape\":(72,72,3)},\n","    {\"filters\":16, \"kernel_size\":(5,5), \"strides\":(2,2), \"padding\":\"same\"},\n","    None,\n","    {\"filters\":32, \"kernel_size\":(3,3), \"padding\":\"same\"},\n","    {\"filters\":32, \"kernel_size\":(3,3), \"padding\":\"same\"},\n","    {\"pool_size\":(2,2), \"strides\":(1,1)},\n","    None,\n","    {\"filters\":64, \"kernel_size\":(3,3), \"padding\":\"same\"},\n","    {\"filters\":64, \"kernel_size\":(3,3), \"padding\":\"same\"},\n","    {\"pool_size\":(2,2), \"strides\":(1,1)},\n","    None,\n","    None,\n","    {\"size\":128, \"activation\":'relu'},\n","    {\"rate\":0.2},\n","    {\"size\":64, \"activation\":'relu'},\n","    {\"rate\":0.2},\n","    {\"size\":1, \"activation\":'sigmoid'}]\n"],"id":"428c1c9a-791d-4eaf-9f98-fc76fab369bc"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":502},"executionInfo":{"elapsed":346640,"status":"ok","timestamp":1658864591368,"user":{"displayName":"Parker Dunn","userId":"12849044322373918721"},"user_tz":240},"id":"rH0RHoIiOPH8","outputId":"13346805-e5cb-4936-91df-1d23c2d064d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/4\n","2304/2304 [==============================] - 88s 37ms/step - loss: 0.4491 - accuracy: 0.8012 - val_loss: 0.5767 - val_accuracy: 0.7020\n","Epoch 2/4\n","2304/2304 [==============================] - 85s 37ms/step - loss: 0.3650 - accuracy: 0.8416 - val_loss: 0.6588 - val_accuracy: 0.7223\n","Epoch 3/4\n","2304/2304 [==============================] - 87s 38ms/step - loss: 0.3287 - accuracy: 0.8608 - val_loss: 0.3856 - val_accuracy: 0.8303\n","Epoch 4/4\n","2304/2304 [==============================] - 87s 38ms/step - loss: 0.3015 - accuracy: 0.8746 - val_loss: 0.3855 - val_accuracy: 0.8406\n","--- Ending Accuracy --- \n"," Validation: 84.06% \n"," Training: 87.46% \n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# *********** CONFIGURE THE DATA FOR PEROFRMANCE ***********************\n","\n","# train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","# ***********  STEPS TO CREATE THE MODEL  *****************************\n","model = model_creator(layers_lst, layer_design)\n","model = model_compiler(model, 0.001, ['accuracy'])\n","\n","# history is kind of like results!\n","# history = model.fit(train_ds,\n","#                     validation_data=val_ds,\n","#                     epochs=10,\n","#                     steps_per_epoch=200\n","#                    )                              # Added steps_per_epoch to give me a little more control of the training process\n","\n","history = model.fit(train_ds,\n","                    validation_data=val_ds,\n","                    epochs=4\n","                    )\n","\n","# ************** VIEWING THE OUTPUTS ***********************\n","print(\"--- Ending Accuracy ---\",\"\\n\",\n","      f\"Validation: {history.history['val_accuracy'][-1]:.2%}\",\"\\n\",\n","      f\"Training: {history.history['accuracy'][-1]:.2%}\",\"\\n\")\n","\n","plt.plot(history.history['accuracy'], label='accuracy')\n","plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0.5, 1])\n","plt.legend(loc='lower right')\n","\n","# *********** SAVING THE MODEL AND RESULTS *****************\n","now_obj = datetime.now()\n","now_date = now_obj.strftime(\"%Y_%m_%d\")\n","now_time = (now_obj.hour * 3600) + (now_obj.minute * 60) + now_obj.second\n","now = f\"{now_date}_{now_time}\"\n","\n","filename1=f\"{now}_{int(history.history['val_accuracy'][-1] * 100)}.json\"\n","filename2=f\"{now}_layers.json\"\n","filename3=f\"{now}_design.json\"\n","\n","\n","with open(f\"{PATH}design_testing/{filename1}\", \"w\") as write_file:\n","    model_json = model.to_json()\n","    json.dump(model_json, write_file)\n","    \n","with open(f\"{PATH}design_testing/{filename2}\", \"w\") as write_file:\n","    json.dump(layers_lst, write_file)\n","\n","with open(f\"{PATH}design_testing/{filename3}\", \"w\") as write_file:\n","    json.dump(layer_design, write_file)"],"id":"rH0RHoIiOPH8"},{"cell_type":"markdown","metadata":{"id":"7m5WNW4vqOq0"},"source":["## Step 3 - Choices for Design\n","\n","After testing many designs, I have decided to use the design below. You can find information about the other designs I have tested in the \"design_testing\" folder. I saved JSON files with information about each design tested.\n","\n","Image Modification Layers:\n","* Adjust pixel intensities from [0,255] to [0,1]\n","* Crop input images to remove 24 px from height and width.\n","\n","Convolution Layers:\n","* Conv2D -> 7x7 kernel & 16 filters\n","* Conv2D -> 5x5 kernel & 16 filters\n","* BatchNormalization\n","* Conv2D -> 3x3 kernel & 32 filters\n","* Conv2D -> 3x3 kernel & 32 filters\n","* MaxPool -> 2x2 region\n","* BatchNormalization\n","* Conv2D -> 3x3 kernel & 64 filters\n","* Conv2D -> 3x3 kernel & 64 filters\n","* MaxPool -> 2x2 region\n","* Batch Normalization\n","\n","Arificial Neural Network:\n","* Dense -> 128\n","* Dropout -> 0.2\n","* Dense -> 64\n","* Dropout -> 0.2\n","* Dense -> 1 (with sigmoid acitvation function)\n","\n","I noticed a couple trends that led me to choose this design. Using the chosen style for convolution layers - i.e., Conv-Conv-Pool, designs with many layers were inconsistent. In early layers, large filters and strides greater than one had better results with regard to training and validation accuracy.\n","\n","It seems likely, with the right additions, that deeper networks would do a better job on this task. The deep ImageNet models like LeNet and ResNet have a lot of success, but I was not able to use the `Sequential` model and include the connections that make these models successful. For the deeper designs tested, I concluded that high variance training and validation accuracy (see data at the beginning of step 4) were the result of the model complexity and ended up with a model with relatively few parameters.\n","\n","Images provided are much larger than the regions used to classify them. My initial approach was to crop most of the unneeded region. Later in the testing process, the model achieved better success when less of the image was cropped out, which was paired with 5x5 and 7x7 filters and larger strides. The larger filters and strides were used in the first Conv-Conv-Pool block of my final design as a result.\n","\n","# NEXT UP: Section 4: Results and Analysis\n","\n","* I will solidify all parameters of the model via hyperparameter tuning.\n","* I will train a model on all data to generate a final model."],"id":"7m5WNW4vqOq0"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"CancerMini-Project_Step3.ipynb","provenance":[],"machine_shape":"hm"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":5}
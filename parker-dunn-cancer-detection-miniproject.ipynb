{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Parker Dunn\npgdunn@bu.edu | pdunn91@gmail.com\n\nCreated on July 1st, 2022\nFinished on August 5th, 2022\n\n\n__Assignment for COURSERA: Introduction to Deep Learning (via CU Boulder)__\n\n__Assignment:__ Week 3 - CNN Cancer Detection Kaggle Mini-Project","metadata":{}},{"cell_type":"markdown","source":"# Step 1 - Description of the data and problem\n\n## First, here are all imports and some important variables","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# Working with files\nimport os\nimport shutil\nimport glob\nimport pathlib\n\n# Data manipulation and linear algebra packages\nimport numpy as np\nimport pandas as pd\n\n# Visualization and image packages\nfrom skimage import io\nimport matplotlib.pyplot as plt\n\n# TensorFlow imports\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\n\n# Other untility packages\nimport json\nfrom datetime import datetime\n\n# PULL THE KAGGLE COMPEITION DATA IN\n# For this notebook, made on Kaggle, I can import the data manually.\n# All data ends up in \"/kaggle/input/\"\n\n        \nPATH=\"/kaggle/working\"\nINPUT=\"/kaggle/input/histopathologic-cancer-detection\"\nINPUT2=\"/kaggle/input/histopathologic-cancer-detection-supplement\"\n\nTRAIN=pathlib.Path(\"/kaggle/working/train/\")\nTEST=pathlib.Path(\"/kaggle/input/histopathologic-cancer-detection/test/\")\n\nTEST_LOCS=os.listdir(TEST)\n\nbatch_size=64\nimg_dim=96\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-08T01:11:39.705653Z","iopub.execute_input":"2022-08-08T01:11:39.706249Z","iopub.status.idle":"2022-08-08T01:11:49.835971Z","shell.execute_reply.started":"2022-08-08T01:11:39.706191Z","shell.execute_reply":"2022-08-08T01:11:49.835011Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Description of the data and problem\n\nFor this project, originally a Kaggle competition, the task is to identify images of cells that contain metastatic cancer. This is a binary classification task on images. If an image contains at least one pixel of tumor tissue within a specified region, it is given a positive label. The images are 96x96 pixels, but the specific region used to classify the images is the center 32x32 pixel region. The task is setup this way to enable fully-convolutional models that do not use padding.\n\nThe data for this task is provided as 96x96 images, which are small parts of larger images. Reading in the three color channels (RGB) for each pixel, this means there are 27,648 total features for each image: the three RGB channels for each pixel and 9,216 pixels. Each pixel is provided as three values from 0 to 255, representing the contribution of each RGB color channel.\n\nThere are 277,483 images provided for this task. The images are divided into 220,025 images for training, since the corresponding labels are provided, and 57,458 images without provided labels. For these test images, a model will try to correctly predict whether these images have (1) or do not have (0) metastatic cancer cells.\n\nSummary:\n* 96x96x3 dimensions for each image -> 27,648 features initially\n* 220,025 images in training set\n* TRAINING: 27648 features x 220,025 images","metadata":{}},{"cell_type":"markdown","source":"## At the end of step 1, I am defining some functions that I will use and loading information about the images/data.","metadata":{}},{"cell_type":"code","source":"def load_image_info(PATH):    \n    train_locs = glob.glob(f\"{PATH}/train/*.tif\")\n    test_locs = glob.glob(f\"{PATH}/test/*.tif\")\n    \n    num_train = len(train_locs)\n    num_test = len(test_locs)\n    \n    y_train = pd.read_csv(f\"{PATH}/train_labels.csv\", header=0)\n    \n    return train_locs, test_locs, y_train\n","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:15:44.007818Z","iopub.execute_input":"2022-08-07T23:15:44.008216Z","iopub.status.idle":"2022-08-07T23:15:44.015289Z","shell.execute_reply.started":"2022-08-07T23:15:44.008178Z","shell.execute_reply":"2022-08-07T23:15:44.013872Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"training_images, testing_images, y_train = load_image_info(INPUT)\n\ny_train.head(8)\n\nprint(\"Double checking the type of the 'id' of the images...\",\n      \"\\n\\n\",\n      y_train.loc[0,'id'],\n      \"\\n\\n\",\n      type(y_train.loc[0,'id']))","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:15:44.017373Z","iopub.execute_input":"2022-08-07T23:15:44.018213Z","iopub.status.idle":"2022-08-07T23:15:45.694035Z","shell.execute_reply.started":"2022-08-07T23:15:44.018175Z","shell.execute_reply":"2022-08-07T23:15:45.692446Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Step 2 - Exploratory Data Analysis (EDA)\n\n## Starting off by writing some functions that I'll use during EDA","metadata":{}},{"cell_type":"code","source":"def show_training_image(PATH, img_info):\n    # displaying the image\n    file = PATH+\"/train/\"+img_info.loc[\"id\"]+\".tif\"\n    image = io.imread(file)\n    plt.imshow(image)\n    plt.title(\"{}\\n Class: {}\".format(img_info.loc[\"id\"], img_info.loc[\"label\"]))\n    \n    # Drawing the center 32x32 region on the picture\n    rectangle = plt.Rectangle((32,32), 32, 32, ec=\"red\", linewidth=1.5, fill=False)\n    plt.gca().add_patch(rectangle)\n    plt.legend([\"Classification Region\"])\n    \ndef show_training_images(PATH, img_info, dim):\n    fig, axes = plt.subplots(nrows=dim[0], ncols=dim[1], figsize=(10,8))\n    \n    ax = axes.flatten()\n    plt.subplots_adjust(hspace=0.4)\n    \n    for i in range(len(img_info.index)):\n        #file = \"train/\" + img_info.loc[i,\"id\"] + \".tif\"\n        file = PATH+\"/train/\"+img_info.loc[i,\"id\"]+\".tif\"\n        image = io.imread(file)\n        ax[i].imshow(image)\n        ax[i].set_title(\"{}\\n Class: {}\".format(img_info.loc[i,\"id\"], img_info.loc[i,\"label\"]))\n        rectangle = plt.Rectangle((32,32), 32, 32, ec=\"red\", linewidth=1.5, fill=False)\n        ax[i].add_patch(rectangle)\n        ax[i].legend([\"Classification Region\"])\n\ndef load_image_data(PATH, img):\n    file = PATH+\"/train/\"+img+\".tif\"\n    image = io.imread(file)\n    return image\n\ndef calculate_img_avgs(images):\n    avg = np.zeros((96,96,3))\n    for img in images:\n        image_np = load_image_data(img)\n        avg = avg + image_np\n    avg = avg/len(images)\n    return avg","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:15:45.697949Z","iopub.execute_input":"2022-08-07T23:15:45.699126Z","iopub.status.idle":"2022-08-07T23:15:45.721895Z","shell.execute_reply.started":"2022-08-07T23:15:45.699078Z","shell.execute_reply":"2022-08-07T23:15:45.720180Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Inspecting a single image\n","metadata":{}},{"cell_type":"code","source":"print(y_train.iloc[0,:])\nprint(type(y_train.iloc[0,:]),\"\\n\\n\")\n\nshow_training_image(INPUT, y_train.iloc[0,:])","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:15:45.723750Z","iopub.execute_input":"2022-08-07T23:15:45.725236Z","iopub.status.idle":"2022-08-07T23:15:45.995387Z","shell.execute_reply.started":"2022-08-07T23:15:45.725187Z","shell.execute_reply":"2022-08-07T23:15:45.993989Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Inspecting multiple images","metadata":{}},{"cell_type":"code","source":"show_training_images(INPUT, y_train.iloc[0:4,:], (2,2))","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:15:45.997680Z","iopub.execute_input":"2022-08-07T23:15:45.998627Z","iopub.status.idle":"2022-08-07T23:15:47.049991Z","shell.execute_reply.started":"2022-08-07T23:15:45.998584Z","shell.execute_reply":"2022-08-07T23:15:47.048867Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Examining information distribution of the images\n\nHere, I looked at...\n* the \"average\" picture\n* comparison of purple primary color distributions\n\n### Average of all training images\n\nOriginally, I used the code below to generate the average image data.\n\n```python\n# loop to collect averages of 100 images at a time\n# the 100 image avgs are saved in a list\navg_img_100 = []\nmoving_avg = np.zeros((96,96,3))\ncounter = 0\n\nfor i in range(len(y_train.index)):\n    if (i % 100 == 0) and (i != 0):\n        moving_avg = moving_avg/100\n        avg_img_100.append(moving_avg)\n        moving_avg = np.zeros((96,96,3))\n        counter = 0\n    if (i % 10000 == 0):\n        print(\"Progress...\")\n    \n    image = load_image_data(y_train.loc[i,\"id\"])\n    counter += 1\n    moving_avg = moving_avg + image\n\nif counter > 0:\n    print(\"End value of counter: \",counter)\n    avg_img_100.append(moving_avg/counter)\n\nprint(\"Number of NumPy arrays saved in avg_img_100: \", len(avg_img_100))\n# print(avg_img_100[0].shape)\n\n# getting the \"average image\" from the averages of 100 images\navg_img = np.zeros((96,96,3))\nfor i in range(len(avg_img_100)):\n    avg_img = avg_img + avg_img_100[i]\n\navg_img = avg_img/len(avg_img_100)\n\nsavable_avg_img = avg_img.reshape(96*96,3)\n\nnp.savetxt(\"/kaggle/working/avg_training_image.txt\", savable_avg_img, delimiter=\",\")\n\nprint(\"Shape of 'avg_img': \", avg_img.shape)\nprint(\"\\nSample values...\\n\", avg_img[0:2,0:2,:])\n\n```\n\nThe code above is very slow. It works to get the information desired, but running it multiple times makes no sense. Since I saved the resulting information in a text file, I will upload the data to this environment and load it from the saved file rather than putting the above code in a code cell and running it again.","metadata":{}},{"cell_type":"code","source":"# Loading the text file data\navg_img = np.genfromtxt(f\"{INPUT2}/avg_training_image.txt\", delimiter=',')\n\n# Transforming the image shape\navg_img = avg_img.reshape(96,96,3)\n\n# Displaying the avg image\navg_img_int = avg_img.astype('int')\nplt.imshow(avg_img_int)\nplt.title(\"Average training image - Both Classes\")","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:15:47.052528Z","iopub.execute_input":"2022-08-07T23:15:47.053376Z","iopub.status.idle":"2022-08-07T23:15:47.410321Z","shell.execute_reply.started":"2022-08-07T23:15:47.053332Z","shell.execute_reply":"2022-08-07T23:15:47.408913Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"I probably should have figured that the avg of all images would not be particularly helpful.\n\nI am curious to see if there is a difference between the average positive vs. negative image. I will basically repeat the same process once more; hopefully, there is some more useful information there.\n\n### Average image with positive and negative images separated (& additionally tracking the average R and B channel values across each image)\n\nOnce again, below is the code that I originally used. However, I am going to load the image data that I saved rather than running this code again.\n\n```python\n# loop to collect averages of 100 images at a time\n# the 100 image avgs are saved in a list\navg_img_pos = []\navg_img_neg = []\nmoving_avg_pos = np.zeros((96,96,3))\nmoving_avg_neg = np.zeros((96,96,3))\ncounter_pos = 0\ncounter_neg = 0\n\n# Simultaneous task -> track and save values for R & B channels of each image\npos_R = []\npos_B = []\nneg_R = []\nneg_B = []\n# End of setup for simultaneous task\n\nfor i in range(len(y_train.index)):\n    if (counter_pos == 1000):\n        moving_avg_pos = moving_avg_pos/counter_pos\n        avg_img_pos.append(moving_avg_pos)\n        moving_avg_pos = np.zeros((96,96,3))\n        counter_pos = 0\n    \n    if (counter_neg == 1000):\n        moving_avg_neg = moving_avg_neg/counter_neg\n        avg_img_neg.append(moving_avg_neg)\n        moving_avg_neg = np.zeros((96,96,3))\n        counter_neg = 0\n    \n    image = load_image_data(y_train.loc[i,\"id\"])\n    #print(image[:,:,0].shape)\n    \n    if (y_train.loc[i,\"label\"] == 1):\n        counter_pos += 1\n        moving_avg_pos = moving_avg_pos + image\n        \n        avg_R = image[:,:,0].reshape((1,-1)).mean()\n        avg_B = image[:,:,2].reshape((1,-1)).mean()\n        pos_R.append(avg_R)\n        pos_B.append(avg_B)\n    else:\n        counter_neg += 1\n        moving_avg_neg = moving_avg_neg + image\n        \n        avg_R = image[:,:,0].reshape((1,-1)).mean()\n        avg_B = image[:,:,2].reshape((1,-1)).mean()\n        neg_R.append(avg_R)\n        neg_B.append(avg_B)\n\nif (counter_pos > 0):\n    avg_img_pos.append(moving_avg_pos/counter_pos)\nif (counter_neg > 0):\n    avg_img_neg.append(moving_avg_neg/counter_neg)\n\navg_pos = np.zeros((96,96,3))\navg_neg = np.zeros((96,96,3))\n\n# POSITIVE IMAGES\nfor i in range(len(avg_img_pos)):\n    avg_pos = avg_pos + avg_img_pos[i]\navg_pos = avg_pos/len(avg_img_pos)\n\n# NEGATIVE IMAGES\nfor j in range(len(avg_img_neg)):\n    avg_neg = avg_neg + avg_img_neg[i]\navg_neg = avg_neg/len(avg_img_neg)\n\nfor img, file in zip([avg_pos, avg_neg], [\"average_positive_image\", \"average_negative_image\"]):\n    print(file, img.shape)\n    np.save(f\"/kaggle/working/{file}\", img)\n```\n\nThe images below are the average images by pixel across all of the negative and positive images.","metadata":{}},{"cell_type":"code","source":"# Loading the average pos and neg images\navg_pos_img = np.load(f\"{INPUT2}/average_positive_image.npy\")\navg_neg_img = np.load(f\"{INPUT2}/average_negative_image.npy\")\n\n# Showing the images\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(12,10), sharey=True)\n\navg_pos_int = avg_pos_img.astype('int')\navg_neg_int = avg_neg_img.astype('int')\n\naxes[0].imshow(avg_pos_int)\naxes[0].set_title(\"Avg Positive Image\")\naxes[1].imshow(avg_neg_int)\naxes[1].set_title(\"Avg Negative Image\")","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:15:47.415873Z","iopub.execute_input":"2022-08-07T23:15:47.416621Z","iopub.status.idle":"2022-08-07T23:15:47.803851Z","shell.execute_reply.started":"2022-08-07T23:15:47.416579Z","shell.execute_reply":"2022-08-07T23:15:47.802533Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"There isn't much to take away from these images. Since they are averages over many images, they look mostly uniform.\n\nThere is a slight difference in the shade of purple and presentation of cell shapes. The shade of purple of the average negative images appears to be slightly darker and more transparent. The average negative image also appears to have some darker spots that are faintly identfiable. Since unhealthy cancer cells lose their distinct oval/circular shape, this indicates the shape of the healthy cells is a fundamental feature of the images that can help me distiguish between the two classes of images.\n\n__Clearly, purple is the dominant color of these images (due to the staining process used for visualization). Next, I will compare how the component colors of purple (red and blue) compare between the positive and negative images.__\n\n### Looking at R and B channel distributions in positive and negative images","metadata":{}},{"cell_type":"code","source":"# LOADING THE IMAGE DATA\npos_images_channel_vals = pd.read_csv(f\"{INPUT2}/positive_images_channel_vals.csv\")\nneg_images_channel_vals = pd.read_csv(f\"{INPUT2}/negative_images_channel_vals.csv\")\n\npos_R = pos_images_channel_vals.R\npos_B = pos_images_channel_vals.B\nneg_R = neg_images_channel_vals.R\nneg_B = neg_images_channel_vals.B\n\n\n# PLOTTING THE IMAGE DATA\nplt.style.use('fivethirtyeight')\nlight_blue=\"#ADD8E6\"\nlight_red='#FFCCCB'\n\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(12,10), sharey=True)\nplt.subplots_adjust(hspace=0.4)\n\naxes[0,0].set_title(\"Positive Images - Red\")\naxes[0,0].set_xlabel(\"Avg Pixel Value (0 - 255)\",fontsize=\"small\")\naxes[0,0].set_ylabel(\"Count\", fontsize=\"small\")\naxes[0,0].set_xlim(left=0,right=255)\naxes[0,0].hist(pos_R, 20, range=(0,255), color=light_red)\n\naxes[0,1].set_title(\"Positive Images - Blue\")\naxes[0,1].set_xlabel(\"Avg Pixel Value (0 - 255)\",fontsize=\"small\")\naxes[0,1].set_ylabel(\"Count\", fontsize=\"small\")\naxes[0,1].set_xlim(left=0,right=255)\naxes[0,1].hist(pos_B, 20, range=(0,255), color=light_blue)\n\naxes[1,0].set_title(\"Negative Images - Red\")\naxes[1,0].set_xlabel(\"Avg Pixel Value (0 - 255)\",fontsize=\"small\")\naxes[1,0].set_ylabel(\"Count\", fontsize=\"small\")\naxes[1,0].set_xlim(left=0,right=255)\naxes[1,0].hist(neg_R, 20, range=(0,255), color=light_red)\n\naxes[1,1].set_title(\"Negative Images - Blue\")\naxes[1,1].set_xlabel(\"Avg Pixel Value (0 - 255)\",fontsize=\"small\")\naxes[1,1].set_ylabel(\"Count\", fontsize=\"small\")\naxes[1,1].set_xlim(left=0,right=255)\naxes[1,1].hist(neg_B, 20, range=(0,255), color=light_blue)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:15:47.805802Z","iopub.execute_input":"2022-08-07T23:15:47.807128Z","iopub.status.idle":"2022-08-07T23:15:48.720581Z","shell.execute_reply.started":"2022-08-07T23:15:47.807087Z","shell.execute_reply":"2022-08-07T23:15:48.719289Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"The cell-like shapes in the average negative image above show up here. The distribution of purple in the negative images appears to be bimodal, but the positive images resemble a normal distribution. When comparing to the average images above, it is important to note that these histograms are averaged across all the pixels of an image. The average images, however, are averaged across all images and show average data for each pixel.\n\nThe histograms suggest that there are two different \"average\" hues of purple among the negative images, while the positive images tend to average out to similar hues of purple. Since the averages of both channels in the negative and positive images appear to be in similar locations, the color distribution does not really provide a way to distiguish between the two classes consistently. The distributions may suggest something about the colors of the features/objects in the images though, which hopefully the CNN model can identify.\n\n\n#### One thing that I did not do...\n\nThe histograms above are really an amalgamation of lots of pixel information. When generating the data, the R and B channels of each image were averaged across an entire image then saved. It would be interesting to investigate/generate histograms of the R and B channels for each pixel across all images. Or, even better, useful information might be available by looking at the avg. value of the R & B channels across sections of the images. These histograms would reveal more detailed information about distribution of objects in the images, which is essentially removed in the plots above because of averaging across an entire image.\n\nBy implementing a convolutional neural network, hopefully, the object level information can be extracted better than the plots created above.\n\n\n# Step 3 - Model Architecture\n\nIn this section, I try a variety of model architectures to find a strong model for classifying the metastatic cancer images.\n\n## First, some code to prepare the data for Steps 3 & 4","metadata":{}},{"cell_type":"code","source":"# RE-ARRANGING THE IMAGE FILES TO BE COMPATIBLE WITH tf.keras.utils.images_from_directory...\n\n!mkdir /kaggle/working/train\n!mkdir /kaggle/working/train/pos\n!mkdir /kaggle/working/train/neg\n\n# Other folders that need to be made\n!mkdir /kaggle/working/design_testing\n!mkdir /kaggle/working/model_tuning\n!mkdir /kaggle/working/trained_model\n\n\ny_info = pd.read_csv(f\"{INPUT}/train_labels.csv\", header=0)\n\nfor i in range(y_info.shape[0]):\n    f = y_info.loc[i,\"id\"]\n    img = io.imread(INPUT+\"/train/\"+f+\".tif\")\n    if y_info.loc[i,\"label\"]:   # \"== 1\"\n        io.imsave(f\"{TRAIN}/pos/\"+f+\".jpg\", img, check_contrast=False, quality=100)\n        #os.remove(INPUT+\"/train/\"+f+\".tif\")\n    else:\n        #os.remove(INPUT+\"/train/\"+f+\".tif\")\n        io.imsave(f\"{TRAIN}/neg/\"+f+\".jpg\", img, check_contrast=False, quality=100)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T01:19:21.969492Z","iopub.execute_input":"2022-08-08T01:19:21.969857Z","iopub.status.idle":"2022-08-08T01:57:33.828934Z","shell.execute_reply.started":"2022-08-08T01:19:21.969826Z","shell.execute_reply":"2022-08-08T01:57:33.827931Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Design testing plan\n\nI'll stick to a simple model. I plan to use the \"building block-style\" Covolution-Convolution-Pooling pattern. Since we previously experimented with the development of neural network architecture, I am hoping to replicate a reliable NN structure from one of the example image classification models from the videos.\n\nLaid out below are my achitecture testing plans as well as some of the thoughts I have regarding the training of my CNN.\n\n__Design parameters and Hyperparameters__\nDecisions\n* I will use ReLU (hidden layers) and sigmoid (output layer) as activation functions. This is not a design parameter that I plan to vary this time.\n* I will primarily use 3x3 convolutional filters\n* As an optimization method, I will stick to SGD with momentum because I am most familiar with SGD\n\n--- __*Update*__ ---\n* Decided to change optimizer: SGD -> Adam\n  - I was looking for ways to improve my results and Adam was suggested\n  - I'm going to use the default values for the parameters that I am not familiar with: \"beta_1\", \"beta_2\", \"epsilon\"\n\nHyperparameters\n* Learning rate\n    * Test: 0.01 | 0.001 | 0.0001 (3 values)\n* Number of epochs (i.e., how much training)\n\nDesign\n* Number of [Conv-Conv-Pool] layers\n    - Test: 2, 3, 4\n* Number of filters to use\n\nPotential ways to improve a struggling model\n* L2 regularization\n* Batch normalization\n\nI plan to use moderate training parameters at first (e.g. learning rate -> 0.001) to experiment and narrow down some viable convolution designs.\n\n## Step 3 - Part 1: Loading the data","metadata":{}},{"cell_type":"code","source":"# Loading the image data\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN,\n    validation_split=0.33,\n    subset=\"training\",\n    seed=123,\n    image_size=(img_dim, img_dim),\n    batch_size=batch_size)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN,\n    validation_split=0.33,\n    subset=\"validation\",\n    seed=123,\n    image_size=(img_dim, img_dim),\n    batch_size=batch_size)\n\nfor image_batch, labels_batch in train_ds:\n    print(image_batch.shape)\n    print(labels_batch.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:33:57.094290Z","iopub.execute_input":"2022-08-07T23:33:57.095209Z","iopub.status.idle":"2022-08-07T23:34:12.791813Z","shell.execute_reply.started":"2022-08-07T23:33:57.095148Z","shell.execute_reply":"2022-08-07T23:34:12.790501Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Step 3 - Part 2: Creating functions to build model and prepare for training\n\nI decided to create functions for the task of creating the model object each for each new design. For me, it created a cleaner setup for the process of modifying the model design and running the training procedure over and over.\n\nAdditionally, here are some decisions that I made regarding the images that are part of all model iterations:\n* I normalized the images to a [0,1] scale\n* I cropped the images since only the middle 32x32 pixel region of the images is used for classification","metadata":{}},{"cell_type":"code","source":"## Functions that take model designs and parameters and turn them into a model object\n\ndef model_creator(layers_lst, layer_design):\n    model = tf.keras.Sequential()\n    model.add(layers.Rescaling(1./255))\n    model.add(layers.Cropping2D(cropping=((12,12),(12,12)), data_format='channels_last'))\n\n    for (l, d) in zip(layers_lst, layer_design):\n        if l == \"input\":\n            model.add(layers.Conv2D(d[\"filters\"], d[\"kernel_size\"], activation='relu', padding=d[\"padding\"], input_shape=d[\"input_shape\"]))\n        elif l == \"conv\":\n            model.add(layers.Conv2D(d[\"filters\"], d[\"kernel_size\"], activation='relu', padding=d[\"padding\"]))\n        elif l == \"maxpool\":\n            model.add(layers.MaxPool2D(d[\"pool_size\"], strides=d[\"strides\"]))\n        elif l == \"flatten\":\n            model.add(layers.Flatten())\n        elif l == \"dense\":\n            model.add(layers.Dense(d[\"size\"], activation=d[\"activation\"]))\n        elif l == \"output\":\n            model.add(layers.Dense(d[\"size\"]))\n        elif l == \"dropout\":\n            model.add(layers.Dropout(d[\"rate\"]))\n        elif l == \"norm\":\n            model.add(layers.BatchNormalization(axis=3))  # not 100 percent sure 3 is correct\n        elif l == \"conv_pool\":\n            model.add(layers.Conv2D(d[\"filters\"], d[\"kernel_size\"], strides=d[\"strides\"], padding=d['padding']))\n        else:\n            raise Exception(\"Invalid layer provided for the model\")\n\n    return model\n\n\ndef model_compiler(model, opt_param, metrics):\n    opt = optimizers.Adam(learning_rate=opt_param)\n    model.compile(optimizer=opt,\n                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                  metrics=metrics)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:34:12.794591Z","iopub.execute_input":"2022-08-07T23:34:12.795540Z","iopub.status.idle":"2022-08-07T23:34:12.810570Z","shell.execute_reply.started":"2022-08-07T23:34:12.795496Z","shell.execute_reply":"2022-08-07T23:34:12.809228Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Step 3 - Part 3: Testing model parameters\n\nThe first code cell includes two lists that I modified to design the model each time.\n\nThe second cell allowed me to train each new design, then visualize and save information about the model.","metadata":{}},{"cell_type":"code","source":"layers_lst = [\"input\",\n              \"conv_pool\",\n              \"norm\",\n              \"conv\",\n              \"conv\",\n              \"maxpool\",\n              \"norm\",\n              \"conv\",\n              \"conv\",\n              \"maxpool\",\n              \"norm\",\n              \"flatten\",\n              \"dense\",\n              \"dropout\",\n              \"dense\",\n              \"dropout\",\n              \"dense\"\n              ]\nlayer_design = [\n    {\"filters\":16, \"kernel_size\":(7,7), \"padding\":\"valid\", \"input_shape\":(72,72,3)},\n    {\"filters\":16, \"kernel_size\":(5,5), \"strides\":(2,2), \"padding\":\"same\"},\n    None,\n    {\"filters\":32, \"kernel_size\":(3,3), \"padding\":\"same\"},\n    {\"filters\":32, \"kernel_size\":(3,3), \"padding\":\"same\"},\n    {\"pool_size\":(2,2), \"strides\":(2,2)},\n    None,\n    {\"filters\":64, \"kernel_size\":(3,3), \"padding\":\"same\"},\n    {\"filters\":64, \"kernel_size\":(3,3), \"padding\":\"same\"},\n    {\"pool_size\":(2,2), \"strides\":(2,2)},\n    None,\n    None,\n    {\"size\":128, \"activation\":'relu'},\n    {\"rate\":0.25},\n    {\"size\":64, \"activation\":'relu'},\n    {\"rate\":0.25},\n    {\"size\":1, \"activation\":'sigmoid'}]","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:34:12.815171Z","iopub.execute_input":"2022-08-07T23:34:12.816359Z","iopub.status.idle":"2022-08-07T23:34:12.838134Z","shell.execute_reply.started":"2022-08-07T23:34:12.816318Z","shell.execute_reply":"2022-08-07T23:34:12.836636Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# ***********  STEPS TO CREATE THE MODEL  *****************************\nmodel = model_creator(layers_lst, layer_design)\nmodel = model_compiler(model, 0.001, ['accuracy'])\n\n\nhistory = model.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs=4\n                    )\n\nmodel.summary()\n\n# ************** VIEWING THE OUTPUTS ***********************\nprint(\"--- Ending Accuracy ---\",\"\\n\",\n      f\"Validation: {history.history['val_accuracy'][-1]:.2%}\",\"\\n\",\n      f\"Training: {history.history['accuracy'][-1]:.2%}\",\"\\n\")\n\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')\n\n# *********** SAVING THE MODEL AND RESULTS *****************\nnow_obj = datetime.now()\nnow_date = now_obj.strftime(\"%Y_%m_%d\")\nnow_time = (now_obj.hour * 3600) + (now_obj.minute * 60) + now_obj.second\nnow = f\"{now_date}_{now_time}\"\n\nfilename1=f\"{now}_{int(history.history['val_accuracy'][-1] * 100)}.json\"\nfilename2=f\"{now}_layers.json\"\nfilename3=f\"{now}_design.json\"\n\n\nwith open(f\"{PATH}/design_testing/{filename1}\", \"w\") as write_file:\n    model_json = model.to_json()\n    json.dump(model_json, write_file)\n    \nwith open(f\"{PATH}/design_testing/{filename2}\", \"w\") as write_file:\n    json.dump(layers_lst, write_file)\n\nwith open(f\"{PATH}/design_testing/{filename3}\", \"w\") as write_file:\n    json.dump(layer_design, write_file)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:34:12.840192Z","iopub.execute_input":"2022-08-07T23:34:12.840661Z","iopub.status.idle":"2022-08-07T23:43:04.748088Z","shell.execute_reply.started":"2022-08-07T23:34:12.840632Z","shell.execute_reply":"2022-08-07T23:43:04.746602Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Choices for my model design\n\nAfter testing many designs, I have decided to use the design below. You can find information about the other designs I have tested in the \"design_testing\" folder. I saved JSON files with information about each design tested.\n\nImage Modification Layers:\n* Adjust pixel intensities from [0,255] to [0,1]\n* Crop input images to remove 24 px from height and width.\n\nConvolution Layers:\n* Conv2D -> 7x7 kernel & 16 filters\n* Conv2D -> 5x5 kernel & 16 filters\n* BatchNormalization\n* Conv2D -> 3x3 kernel & 32 filters\n* Conv2D -> 3x3 kernel & 32 filters\n* MaxPool -> 2x2 region\n* BatchNormalization\n* Conv2D -> 3x3 kernel & 64 filters\n* Conv2D -> 3x3 kernel & 64 filters\n* MaxPool -> 2x2 region\n* Batch Normalization\n\nArificial Neural Network:\n* Dense -> 128\n* Dropout -> 0.2\n* Dense -> 64\n* Dropout -> 0.2\n* Dense -> 1 (with sigmoid acitvation function)\n\nI noticed a couple trends that led me to choose this design. Using the chosen style for convolution layers - i.e., Conv-Conv-Pool, designs with many layers were inconsistent. In early layers, large filters and strides greater than one had better results with regard to training and validation accuracy.\n\nIt seems likely, with the right additions, that deeper networks would do a better job on this task. The deep ImageNet models like LeNet and ResNet have a lot of success, but I was not able to use the `Sequential` model and include the connections that make these models successful. For the deeper designs tested, I concluded that high variance training and validation accuracy (see data at the beginning of step 4) were the result of the model complexity and ended up with a model with relatively few parameters.\n\nImages provided are much larger than the regions used to classify them. My initial approach was to crop most of the unneeded region. Later in the testing process, the model achieved better success when less of the image was cropped out, which was paired with 5x5 and 7x7 filters and larger strides. The larger filters and strides were used in the first Conv-Conv-Pool block of my final design as a result.\n\n#### Next up in section 4\n\n* I will solidify all parameters of the model via hyperparameter tuning.\n* I will train a model on all data to generate a final model.","metadata":{}},{"cell_type":"markdown","source":"# Step 4 - Results and Analysis\n\n## Part 1 - Displaying designs tested and analyzing the CNN model\n\nBefore moving on to hyperparameter tuning, I have included a brief summary of the designs that I tested below. Then, I discuss some of the techniques used to improve the overall success of my model design (- I tested these in the previous step while testing other model design features).","metadata":{}},{"cell_type":"code","source":"cnn_designs = pd.read_csv(f\"{INPUT2}/cnn_designs_tested.csv\")\nprint(cnn_designs)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:43:04.750341Z","iopub.execute_input":"2022-08-07T23:43:04.750707Z","iopub.status.idle":"2022-08-07T23:43:04.786515Z","shell.execute_reply.started":"2022-08-07T23:43:04.750676Z","shell.execute_reply":"2022-08-07T23:43:04.785104Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"I introduced some normalization techniques toward the end of my design testing procedure.\n\nFirst, I started introducing batch normalization into the convolution layers of the model. I noticed initial improvement compared to the similar designs that I had already tested and decided to include a batch noramlization layer in each Conv-Conv-Pool block. I did not extensively experiment with the effects of batch normalization though.\n\nSecond, I introduced dropout layers in the neural network. I tried dropout rates between 0.1 and 0.5. Designs with lots of parameters had better results with rates around 0.3, but slightly lower rates worked better for models of the size that I chose in the end. Thus, I use 0.2 as the dropout rate in both of the dropout layers.\n\n## Part 2 - Preparations for hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"LEARNING_RATES = [0.0001, 0.001, 0.01]\nN_EPOCHS = 6\n\n# FUNCTION TO CREATE MODEL AND TRAIN - allows me to easily test hyperparameters\n\ndef train_model(train_ds, val_ds, learning_rate, num_epochs):\n    model = tf.keras.Sequential([\n        layers.Rescaling(1./255),\n        layers.Cropping2D(cropping=((12,12),(12,12)), data_format=\"channels_last\"),\n        layers.Conv2D(16, (7,7), activation='relu', padding='valid', input_shape=(72,72,3)),\n        layers.Conv2D(16, (5,5), activation='relu', strides=(2,2), padding='same'),\n        layers.BatchNormalization(axis=-1),\n        layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n        layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n        layers.MaxPool2D((2,2), strides=(2,2)),\n        layers.BatchNormalization(axis=-1),\n        layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n        layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n        layers.MaxPool2D((2,2), strides=(2,2)),\n        layers.BatchNormalization(axis=-1),\n        layers.Flatten(),\n        layers.Dense(128, activation='relu'),\n        layers.Dropout(0.25),\n        layers.Dense(64, activation='relu'),\n        layers.Dropout(0.25),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    opt = optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=opt,\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                  metrics=['accuracy'])\n\n    results = model.fit(train_ds, verbose=0, validation_data=val_ds, epochs=num_epochs)\n\n    return results, model","metadata":{"execution":{"iopub.status.busy":"2022-08-08T01:57:33.844128Z","iopub.execute_input":"2022-08-08T01:57:33.844875Z","iopub.status.idle":"2022-08-08T01:57:33.859197Z","shell.execute_reply.started":"2022-08-08T01:57:33.844839Z","shell.execute_reply":"2022-08-08T01:57:33.858080Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Part 3 - Tuning hyperparameters","metadata":{}},{"cell_type":"code","source":"results = []\n\n# Retrieving information so I can save model results\nnow_obj = datetime.now()\nnow_date = now_obj.strftime(\"%Y_%m_%d\")\nnow_time = (now_obj.hour * 3600) + (now_obj.minute * 60) + now_obj.second\nnow = f\"{now_date}_{now_time}\"\n\ncounter=0\nfor eta in LEARNING_RATES:\n    # Train a model with the selected learning rate\n    run_results, model = train_model(train_ds, val_ds, eta, N_EPOCHS)\n    if counter < 1:\n        model.summary()\n        counter += 1\n    # Save results for plotting later\n    results.append(run_results)\n    # Saving model info\n    filename = f\"{PATH}/model_tuning/{now}_model_{eta}\"\n    model.save(filename)  # all other parameters default to the values I wanted\n\n# Viewing the results\nfig, axes = plt.subplots(figsize=(12,6), nrows=1, ncols=3, sharey=True)\nplt.subplots_adjust(wspace=0.2)\n\nfor i in range(len(LEARNING_RATES)):\n    axes[i].plot(results[i].history['accuracy'], label='accuracy')\n    axes[i].plot(results[i].history['val_accuracy'], label='val_accuracy')\n    axes[i].set_xlabel(\"Epoch\")\n    axes[i].set_ylim([0.5,1])\n    axes[i].legend(loc=\"lower right\")\n\n    val_accuracy = results[i].history['val_accuracy'][-1]\n    train_accuracy = results[i].history['accuracy'][-1]\n    title = f\"Learning Rate: {LEARNING_RATES[i]}\\nTrain: {train_accuracy:.2%} - Val: {val_accuracy:.2%}\"\n    axes[i].set_title(title)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T23:43:04.806049Z","iopub.execute_input":"2022-08-07T23:43:04.806483Z","iopub.status.idle":"2022-08-08T00:17:42.971916Z","shell.execute_reply.started":"2022-08-07T23:43:04.806435Z","shell.execute_reply":"2022-08-08T00:17:42.970559Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Part 4 - Training final model","metadata":{}},{"cell_type":"code","source":"# DEBUGGING CELL - PLEASE IGNORE\n!mkdir /kaggle/working/trained_model\nmodel.save(filename)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T02:17:00.991276Z","iopub.execute_input":"2022-08-08T02:17:00.991856Z","iopub.status.idle":"2022-08-08T02:17:04.810606Z","shell.execute_reply.started":"2022-08-08T02:17:00.991825Z","shell.execute_reply":"2022-08-08T02:17:04.809579Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"full_train_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN,\n    image_size=(img_dim, img_dim),\n    batch_size=batch_size)\n\nresults, model = train_model(full_train_ds, None, learning_rate=0.0001, num_epochs=4)\n\n# Retrieving information so I can save model results\nnow_obj = datetime.now()\nnow_date = now_obj.strftime(\"%Y_%m_%d\")\nnow_time = (now_obj.hour * 3600) + (now_obj.minute * 60) + now_obj.second\nnow = f\"{now_date}_{now_time}\"\n\nfilename = f\"{PATH}/trained_model/{now}_model\"\nmodel.save(filename)\n\n# Viewing the training results\nplt.plot(results.history['accuracy'], label='accuracy')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.ylim([0.5, 1])\ninfo = results.history['accuracy'][-1]\nplt.title(f\"Final Accuracy: {info:0.2%}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-08T02:09:16.820668Z","iopub.execute_input":"2022-08-08T02:09:16.821490Z","iopub.status.idle":"2022-08-08T02:14:44.783946Z","shell.execute_reply.started":"2022-08-08T02:09:16.821453Z","shell.execute_reply":"2022-08-08T02:14:44.783023Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Part 5 - Testing the model on the test dataset\n\n#### First, some helper functions","metadata":{}},{"cell_type":"code","source":"def load_test_image(FILE_PATH):\n    img = io.imread(FILE_PATH)\n    return img\n\ndef load_batch(FILES_PATH, FILES):\n    n = len(FILES)\n    imgs = np.zeros((n,96,96,3), dtype=np.int8)\n    for i in range(n):\n        img = io.imread(str(FILES_PATH)+\"/\"+FILES[i])\n        imgs[i,:,:,:] = img\n    return imgs\n\ndef evaluate_test_images(model, FILES_PATH, img_files):\n    image_ids = []\n    raw_outputs = []\n    results = []\n\n    iter = 0\n    while (len(img_files) - iter) > 128:\n        batch_files = img_files[iter:(iter+64)]\n        imgs = load_batch(TEST, batch_files)\n        batch_img_ids = [f[:-4] for f in batch_files]\n\n        iter += 64\n        image_ids.extend(batch_img_ids)\n        \n        batch_result = model.predict(imgs)\n        raw_outputs.extend(batch_result[:,0])\n        batch_result = np.where(batch_result > 0.5, 1, 0)\n        results.extend(batch_result[:,0])\n    \n\n    batch_files = img_files[iter:]\n    imgs = load_batch(TEST, batch_files)\n    batch_img_ids = [f[:-4] for f in batch_files]\n\n    image_ids.extend(batch_img_ids)\n    \n    batch_result = model.predict(imgs)\n    raw_outputs.extend(batch_result[:,0])\n    batch_result = np.where(batch_result > 0.5, 1, 0)\n    results.extend(batch_result[:,0])\n\n    return pd.DataFrame({'id':image_ids, 'label':results, 'raw_output':raw_outputs})\n\n# TESTING \"load_batch\" FUNCTION\nsample_batch = load_batch(TEST, TEST_LOCS[0:64])\nprint(sample_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T02:35:31.653796Z","iopub.execute_input":"2022-08-08T02:35:31.654172Z","iopub.status.idle":"2022-08-08T02:35:31.764834Z","shell.execute_reply.started":"2022-08-08T02:35:31.654139Z","shell.execute_reply":"2022-08-08T02:35:31.763918Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"#### Now, loading the model and evaluating the test images\n\nLocation of trained model: `/kaggle/working/trained_model/...`\n\nA function was prepared above to assist with loading and evaluating each test image.","metadata":{}},{"cell_type":"code","source":"# DEBUGGING CELL - PLEASE IGNORE\nworking_files = os.listdir(PATH)\nprint(len(working_files))\nfor i in range(6):\n    print(working_files[i])\n    \nmodel_file = os.listdir(\"/kaggle/working/trained_model\")\nprint(model_file)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T02:19:50.702768Z","iopub.execute_input":"2022-08-08T02:19:50.703441Z","iopub.status.idle":"2022-08-08T02:19:50.710628Z","shell.execute_reply.started":"2022-08-08T02:19:50.703404Z","shell.execute_reply":"2022-08-08T02:19:50.709423Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model(f\"{PATH}/trained_model/2022_08_08_8081_model\")\n\nmodel.summary()\n\nresults = evaluate_test_images(model, TEST, TEST_LOCS)\n\nresults.head(20)\n\n# SAVING THE TESTING RESULTS AND COUNTING THE NUMBER OF 1s/0s\ntest_counts = results.loc[:,\"label\"].value_counts()\nprint(test_counts)\n\nresults.loc[:,[\"id\",\"label\"]].to_csv(f\"{PATH}/submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T02:35:37.563688Z","iopub.execute_input":"2022-08-08T02:35:37.564084Z","iopub.status.idle":"2022-08-08T02:37:30.987226Z","shell.execute_reply.started":"2022-08-08T02:35:37.564050Z","shell.execute_reply":"2022-08-08T02:37:30.986265Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Part 6 - Model optimization and discussion of testing\n\n### Hyperparameter optimization\n\nI used similar procedure for hyperparemeter testing that I did for testing model designs. \n\nI divided up the training images into training and validation sets. I trained the model (with my chosen design) using three learning rates and 5 epochs each time. I assessed the training process by plotting the training and validation accuracy across the 5 epochs.\n\nAnalayzing the training graphs, I selected a learning rate that appeared to improve training and validation accuracy consistently. I was looking for a training procedure that would yield gradual improvements toward an optimal solution rather than high accuracy results that might overfit the training and validation data.\n\n### Analysis of model testing\n\nI was able to train a model that achieved greater than 80% training and validation accuracy using a learning rate of 0.001 and 4 epochs. This seemed to be the best combination of parameters to use for training.\n\nAfter preparing the model on all the training data, the model achieved greater than 90% accuracy during the last epoch. However, when evaluating on the test data, the overall score was very low -- approx. 0.51.\n\nMy initial idea to explain this low test performance is overfitting to the images that I have been experimenting and training with over and over. I have not worked with many datasets as large as this, so I was unsure about how much data to use at each stage of model development. Normally, it is standard to use as much data as possible. Images of cells, in my opinion, are not very complex though and tend to be repetitive, so I was consistently concerned about overfitting and model complexity.","metadata":{}},{"cell_type":"markdown","source":"# Step 5 - Conclusion\n\nI prematurely ended my efforts to develop a great model. I wanted to spend a reasonable amount of time and set reasonable expectations, so I stopped tweaking my design even though I knew I could probably achieve better. For my first significant deep learning project, it seemed too drastic to try to re-invent a model like ResNet or LeNet. As a result, the testing performance of my model was not great (see image with my Kaggle results - \"parkerdunn_kaggle_submission.jpg\").\n\nIt was a great experience despite the poor results. This was one of the largest ML projects that I pursued so far with regard to size of the model, size of the data, and time invested into model development. It was a valuable lesson about the the model development process, and I know that I will achieve better models in the future as I leave myself more time to work on tuning and design testing. I found myself at times struggling at non-essential steps do to lack of experience and familiarity with this process. For example, I knew I would not be able to load all of the images into a table format at once (I tried anyway, but was pretty sure it wouldn't work), but it took me some time to figure out a strong way to work with the images. I feel like I have a much better sense now of how to prepare and work with data for a deep learning project.\n\nI gained a much better appreciation for regularization techniques during this project. In previous work, I hadn't worked with a dataset and deep model that demonstrated the importance of dropout layers. In this project, it was evident during the training and valudation (while testing designs) that dropout layers did help with overfitting.\n\nAnother big lesson that I learned was to setup consistent ways to visualize model design and performance. In addition to realizing the ease and value of plotting performance, I prohibited my progress at times by not double-checking my model designs with `model.summary()`. It seemes to me that there is no reason not to use it each time a model is created for testing, and I will certainly use it in the future to avoid designing a model in an unintended manner.\n","metadata":{}}]}